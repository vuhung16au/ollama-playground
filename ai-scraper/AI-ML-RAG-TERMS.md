# Important AI/ML and RAG Terms for Beginners

| Term                | Explanation |
|---------------------|-------------|
| **AI (Artificial Intelligence)** | The field of creating machines or software that can perform tasks that typically require human intelligence, such as understanding language, recognizing images, or making decisions. |
| **ML (Machine Learning)** | A subset of AI where computers learn from data to make predictions or decisions without being explicitly programmed for each task. |
| **Deep Learning**   | A type of machine learning that uses neural networks with many layers ("deep" networks) to learn complex patterns in data. |
| **Neural Network**  | A computational model inspired by the human brain, made up of layers of interconnected nodes (neurons) that process data. |
| **Large Language Model (LLM)** | A very large neural network trained on massive amounts of text data to understand and generate human-like language (e.g., GPT, Llama). |
| **Embedding**       | A way to represent words, sentences, or documents as vectors (lists of numbers) so that similar meanings are close together in this numerical space. |
| **Vector Store / Vector Database** | A special database that stores embeddings and allows fast similarity search to find the most relevant documents or text chunks. |
| **Similarity Search** | The process of finding items (like text chunks) in a database that are most similar to a given query, usually by comparing their embeddings. |
| **Prompt**          | The input or instruction given to an AI model to guide its response (e.g., a question or a command). |
| **Prompt Engineering** | The practice of designing and refining prompts to get better or more accurate responses from AI models. |
| **Retrieval-Augmented Generation (RAG)** | A technique where an AI model retrieves relevant information from a database or documents and then uses that information to generate a more accurate answer. |
| **Chunking**        | Breaking up large documents into smaller pieces (chunks) so they can be processed by AI models that have input size limits. |
| **Context Window**  | The maximum amount of text (in tokens or words) that an AI model can process at one time. |
| **Token**           | A unit of text (like a word or part of a word) used by language models. Models have limits on how many tokens they can handle at once. |
| **Fine-tuning**     | Training a pre-existing AI model on new, specific data to make it perform better for a particular task or domain. |
| **Zero-shot / Few-shot Learning** | The ability of a model to perform tasks it hasn't been explicitly trained for, using just a few (few-shot) or no (zero-shot) examples in the prompt. |
| **Inference**       | The process of using a trained AI model to make predictions or generate outputs (like answering a question). |
| **Corpus**          | A large collection of text or documents used for training or evaluating AI models. |
| **Ground Truth**    | The correct or real answer used to evaluate how well an AI model is performing. |
| **Hallucination**   | When an AI model generates information that sounds plausible but is actually false or made up. |
| **Knowledge Base**  | A structured set of information (like documents or facts) that an AI system can use to answer questions. |
| **API (Application Programming Interface)** | A way for different software systems to communicate with each other, often used to access AI models or databases. |
| **Latency**         | The time it takes for a system to process a request and return a result. Lower latency means faster responses. |
| **Throughput**      | The number of tasks or requests a system can handle in a given amount of time. |
| **Evaluation Metric** | A standard way to measure how well an AI model is performing (e.g., accuracy, F1 score). |
| **Overfitting**     | When a model learns the training data too well, including its noise or errors, and performs poorly on new, unseen data. |
| **Underfitting**    | When a model is too simple to capture the patterns in the data, resulting in poor performance. | 